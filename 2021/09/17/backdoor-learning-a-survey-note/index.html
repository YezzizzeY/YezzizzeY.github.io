<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>backdoor learning-a survey note | Yezzi Tech</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="a survey of backdoor learning">
<meta property="og:type" content="article">
<meta property="og:title" content="backdoor learning-a survey note">
<meta property="og:url" content="https://yezzi.tech/2021/09/17/backdoor-learning-a-survey-note/index.html">
<meta property="og:site_name" content="Yezzi Tech">
<meta property="og:description" content="a survey of backdoor learning">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://yezzi.tech/images/backdoor-note/1.png">
<meta property="og:image" content="https://yezzi.tech/images/backdoor-note/2.png">
<meta property="article:published_time" content="2021-09-16T23:54:54.000Z">
<meta property="article:modified_time" content="2021-09-17T00:16:19.467Z">
<meta property="article:author" content="Mingzhe Liu">
<meta property="article:tag" content="backdoor attack">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yezzi.tech/images/backdoor-note/1.png">
  
    <link rel="alternate" href="/atom.xml" title="Yezzi Tech" type="application/atom+xml">
  
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" integrity="sha384-XdYbMnZ/QjLh6iI4ogqCTaIjrFk87ip+ekIjefZch0Y+PvJ8CDYtEs1ipDmPorQ+" crossorigin="anonymous">

  
<link rel="stylesheet" href="/css/styles.css">

  

<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <nav class="navbar navbar-inverse">
  <div class="container">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#main-menu-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="main-menu-navbar">
      <ul class="nav navbar-nav">
        
          <li><a class=""
                 href="/index.html">Home</a></li>
        
          <li><a class=""
                 href="/archives/">Archives</a></li>
        
      </ul>

      <!--
      <ul class="nav navbar-nav navbar-right">
        
          <li><a href="/atom.xml" title="RSS Feed"><i class="fa fa-rss"></i></a></li>
        
      </ul>
      -->
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

  <div class="container">
    <div class="blog-header">
  <h1 class="blog-title">Yezzi Tech</h1>
  
</div>

    <div class="row">
        <div class="col-sm-8 blog-main">
          <article id="post-backdoor-learning-a-survey-note" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 class="article-title" itemprop="name">
      backdoor learning-a survey note
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2021/09/17/backdoor-learning-a-survey-note/" class="article-date"><time datetime="2021-09-16T23:54:54.000Z" itemprop="datePublished">2021-09-17</time></a>
</div>

    
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>a survey of backdoor learning</p>
<span id="more"></span>

<h1 id="Backdoor-learning-—-A-Survey"><a href="#Backdoor-learning-—-A-Survey" class="headerlink" title="Backdoor learning — A Survey"></a>Backdoor learning — A Survey</h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><ul>
<li>Over the past decade, deep neural networks (DNNs) have  been successfully applied tin many mission-critical tasks, such as face recognition, autonomous driving, etc.</li>
<li>Compared to the inference stage, the training stage of DNNs involves more steps,  including data collection, data pre-processing, model selection and  construction, training, model saving, model deployment, etc. More steps mean  more chances for the attacker</li>
<li>Users choose to adopt third-party datasets, rather than to collect the training data by themselves</li>
<li>Users train DNNs based on third-party models</li>
<li></li>
</ul>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>In general, backdoor attacks aim at embedding the hidden backdoor into DNNs so  that the infected model performs well on benign testing samples when the  backdoor is not activated, similarly to the model trained under benign settings;  however, if the backdoor is activated by the attacker, then its prediction will be  changed to the attacker-specified target label.</p>
<p>Currently, training data poisoning is the most straightforward and  common way to encode backdoor functionality into the model’s weights during the  training process.</p>
<p>To alleviate the backdoor threat, different defense methods were proposed. In  general, those methods can be divided into two main categories, including  empirical backdoor defenses and certified backdoor defenses.</p>
<p>Besides, backdoor triggers could be invisible, and except by directly poisoning the training samples, the hidden backdoor could  also be embedded through transfer learning, directly modifying  model’s weights</p>
<p>To alleviate the backdoor threat, different defense methods were proposed. In  general, those methods can be divided into two main categories, including  empirical backdoor defenses and certified backdoor defenses.</p>
<h2 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h2><h3 id="Technical-Terms"><a href="#Technical-Terms" class="headerlink" title="Technical Terms"></a>Technical Terms</h3><ul>
<li><em><strong>Benign model</strong></em> refers to the model trained under benign settings.</li>
<li><em><strong>Infected model</strong></em> refers to the model with hidden backdoor(s).</li>
<li><em><strong>Poisoned sample</strong></em> is the modified training sample used in poisoning-based backdoor  attacks for embedding backdoor(s) in the model during the training process.</li>
<li><em><strong>Trigger</strong></em> is the pattern used for generating poisoned samples and activating the  hidden backdoor(s).</li>
<li>Attacked sample indicates the malicious testing sample (with trigger) used for  querying the infected model.</li>
<li><em><strong>Attack scenario</strong></em> refers to the scenario that the backdoor attack might happen.  Usually, it happens when the training process is inaccessible or out of control  by the user, such as training with third-party datasets, training through  third-party platforms, or adopting third-party models.  </li>
<li><em><strong>Source label</strong></em> indicates  the ground-truth label of a poisoned or an attacked sample. </li>
<li><em><strong>Target label</strong></em> is  the attacker-specified label. The attacker intends to make all attacked samples  to be predicted as the target label by the infected model. </li>
<li><em><strong>Attack success rate  (ASR)</strong></em> denotes the proportion of attacked samples which are predicted as the  target label by the infected model. </li>
<li><em><strong>Benign accuracy (BA)</strong></em> indicates the  accuracy of benign test samples predicted by the infected model. </li>
<li><em><strong>Attacker’s  goal</strong></em> describe what the backdoor attacker intends to do. In general, the attacker  wishes to design an infected model that performs well on the benign testing  sample while achieving high ASR. </li>
<li><em><strong>Capacity</strong></em> defines what the attacker/defender  can and cannot do to achieve their goal. </li>
<li><em><strong>Attack/Defense approach</strong></em> illustrates  the process of the designed backdoor attack/defense.</li>
</ul>
<h3 id="Classical-Scenarios-and-Corresponding-Capacities"><a href="#Classical-Scenarios-and-Corresponding-Capacities" class="headerlink" title="Classical Scenarios and Corresponding Capacities"></a>Classical Scenarios and Corresponding Capacities</h3><p>Scenario 1: Adopt Third-Party Datasets</p>
<p>Scenario 2: Adopt Third-Party Platforms</p>
<p>Scenario 3: Adopt Third-Party Models</p>
<h2 id="A-Unified-Framework-of-Poisoning-based-Attacks"><a href="#A-Unified-Framework-of-Poisoning-based-Attacks" class="headerlink" title="A Unified Framework of Poisoning-based Attacks"></a>A Unified Framework of Poisoning-based Attacks</h2><p>poisoning based attacks can be categorized based on different criteria</p>
<p><img src="/images/backdoor-note/1.png"></p>
<p>Two classical metrics are usually adopted: (1) benign accuracy (BA) and (2) attack success rate (ASR)</p>
<p>The higher the ASR and the smaller the poisoning rate (i.e., rate of poisoned samples over all training samples) and the perturbation between the benign image and the poisoned image, the more stealthy the attack and therefore the better the attack.</p>
<h2 id="Poisoning-Based-Backdoor-Attacks"><a href="#Poisoning-Based-Backdoor-Attacks" class="headerlink" title="Poisoning-Based Backdoor Attacks"></a>Poisoning-Based Backdoor Attacks</h2><h3 id="Attacks-for-Image-and-Video-Recognition"><a href="#Attacks-for-Image-and-Video-Recognition" class="headerlink" title="Attacks for Image and Video Recognition"></a>Attacks for Image and Video Recognition</h3><ul>
<li>Gu et al. [7] first introduced the backdoor attack in deep learning and proposed  a method, dubbed BadNets: <strong>stamping the backdoor trigger onto the benign image.</strong> the trained DNN will be infected, which performs well on benign testing samples, however, if the same  trigger is contained in an attacked image, then its prediction will be changed  to the target label.</li>
<li>Chen et al. : the invisibility requirement of poisoning-based backdoor attacks. <strong>Generated poisoned images by blending the backdoor trigger with benign images  instead of by stamping.</strong> Further reduces the risk of being detected.</li>
<li>Zhong et al. [12] adopted the universal adversarial attack</li>
<li>Li et al. [11] proposed to regularize the norm of the perturbation when optimizing the backdoor trigger.</li>
<li>Bagdasaryan et al. viewed the backdoor attack as a special multi-task  optimization</li>
<li>Liu et al. [8] proposed to adopt a common phenomenon, the reflection, as the  trigger for the stealthiness.</li>
<li>Cheng et al. [29] proposed to conduct the invisible attack in the feature space  in a style transfer way.</li>
<li>Most recently, Li et al. [30] adopted DNN-based image steganography to generate  invisible triggers for the backdoor attack.</li>
</ul>
<hr>
<p>An invisible attack still could be detected by humans by examining the  image-label relationship of training samples. To address this problem, a special sub-class of <strong>invisible poisoning-based  attacks</strong>, dubbed clean-label invisible attacks, was proposed</p>
<ul>
<li>Turner et al. [10] first explored the clean-label attack, where they <strong>leveraged  adversarial perturbations or generative models to first modify some benign  images</strong> from the target class and then conducted the standard invisible attack. - Alleviate the effects of ‘robust features’ contained in the poisoned samples to  ensure that the trigger can be successfully learned by the DNNs.</li>
<li>Zhao et al adopted universal perturbation instead of a given one as the trigger pattern.</li>
<li>Quiring et al.  proposed to conceal the trigger as well as hide the overlays  of clean-label poisoning through image-scaling attacks.</li>
</ul>
<p>clean-label attacks usually suffered from <strong>significantly lower attack success  rate (ASR) and even fail to succeed</strong>. How to balance the stealthiness and effectiveness of attacks is still an open  question and worth further exploration.</p>
<hr>
<p>Triggers are the core of poisoning based attacks, therefore analyzing how to  design a better trigger instead of simply using a given non-optimized trigger  pattern is of great significance</p>
<ul>
<li>Liu et al. proposed to optimize the trigger so that the important neurons can achieve the  maximum values.</li>
<li>Li et al. formulated the trigger generation as a bilevel optimization</li>
<li>Bagdasaryan et al. treated backdoor attacks as a multiobject optimization and proposed to optimize trigger and train DNNs simultaneously.</li>
<li>Recently, with the hypothesis that if a perturbation can induce most samples  toward the decision boundary of the target class then it will serve as an  effective trigger,  proposed to generate trigger through  universal adversarial perturbation.</li>
</ul>
<p>Physical Backdoor Attacks…</p>
<p>Black-box Backdoor Attacks: Different from previous white-box attacks, which  require the knowledge of training samples, black-box attacks adopt the settings  that the training set is inaccessible.</p>
<p>Semantic Backdoor Attacks:  a semantic part of samples can also serve as the trigger, such that the attacker  is not required to modify the input at inference time to deceive the infected  model,  the hidden backdoor can be activated by the combination of certain objects in the image. Since <strong>this type of attacks does not require modifying images in the inference  process</strong> in the digital space, we believe it is very malicious and worth further  exploration.</p>
<h3 id="Attacks-against-Other-Fields-or-Paradigms"><a href="#Attacks-against-Other-Fields-or-Paradigms" class="headerlink" title="Attacks against Other Fields or Paradigms"></a>Attacks against Other Fields or Paradigms</h3><p>Accordingly, except for task-specific requirements, most methods focused on </p>
<p>(1)  how to design the trigger, </p>
<p>(2) how to define the attack stealthiness, and </p>
<p>(3)  how to bypass potential defenses.</p>
<p>Some existing backdoor attacks discussed:</p>
<ul>
<li><p><strong>Natural language processing</strong> is the most extensive research field in backdoor  attacks besides image or video classification. Dai et al. discussed how to attack against LSTM-based sentiment analysis. Chen et al. further explored this problem, where three different types of  triggers were proposed and reached decent performance. Kurita et al. [16] demonstrated that sentiment classification, toxicity  detection, and spam detection can also be attacked even after fine-tuning. Chan et al. [39] proposed to attack NLP models in the latent space based on the  conditional adversarially regularized auto-encoder.</p>
</li>
<li><p>Some researches also revealed the backdoor threat towards <strong>graph neural networks  (GNN)</strong></p>
</li>
<li><p><strong>reinforcement learning</strong></p>
</li>
<li><p><strong>speaker verification</strong></p>
</li>
<li><p><strong>wireless signal classification</strong></p>
<p>how to backdoor collaborative learning, especially federated learning, have also attracted the most attention.</p>
</li>
<li><p>In collaborative learning, <strong>Federated learning have attracted the most attention</strong>. Bagdasaryan et al.  introduced the first backdoor attack against federated learning by amplifying  the poisoned gradient of node servers.Bhagoji et al. discussed the stealthy model-poisoning backdoor attack, and Xie  et al. introduced a distributed backdoor attacks against the federated learning.  Most recently, theoretically verified that backdoor attacks are unavoidable if a  model is vulnerable to adversarial examples under mild conditions in federated  learning.</p>
</li>
<li><p>Besides, the backdoor attacks towards </p>
</li>
<li><p><strong>meta federated learning</strong> and </p>
</li>
<li><p><strong>feature partitioned  collaborative learning</strong> were also discussed. Some works also questioned whether federal learning is really easy to be  attacked.</p>
</li>
<li><p>the backdoor threat of another important learning paradigm, the <strong>transfer  learning</strong>, was also discussed</p>
</li>
</ul>
<p>Backdoor Attack for Good: Except for malicious purposes, how to use the backdoor attack in the right way  has also obtained some preliminary explorations.</p>
<h2 id="NON-POISONING-BASED-BACKDOOR-ATTACKS"><a href="#NON-POISONING-BASED-BACKDOOR-ATTACKS" class="headerlink" title="NON-POISONING-BASED BACKDOOR ATTACKS"></a>NON-POISONING-BASED BACKDOOR ATTACKS</h2><p>Some non-poisoningbased attack methods inject backdoor not directly through optimizing model parameters during  the training process with poisoned samples.</p>
<ul>
<li><strong>Targeted Weight Perturbation:</strong> Dumford et al. first explored the non-poisoningbased attack, where <strong>they proposed  to modify the model’s parameters directly</strong> instead of through training with  poisoned samples.</li>
<li><strong>Targeted Bit Trojan:</strong> Rakin et al. demonstrated a new method, dubbed targeted bit trojan (TBT),  discussing how to <strong>inject a hidden backdoor without the training process</strong>.</li>
<li><strong>TrojanNet</strong>: Guo et al.proposed TrojanNet to encode the backdoor in the infected DNNs  activated through a secret weight permutation. They assumed that the infected network is used with a hidden backdoor software  which could <strong>permute the parameters when the backdoor trigger is presented</strong>.</li>
<li><strong>Attack with Trojan Module</strong>: Tang et al. proposed a novel nonpoisoning-based backdoor attack, which <strong>inserted  a trained malicious backdoor module (i.e., a sub-DNN) into the target model</strong>  instead of changing parameters in the original model to embed hidden backdoors.</li>
</ul>
<p><strong>Similarities between poisoning-based backdoor attacks and data poisoning:</strong></p>
<p>they all aim at misleading models in the inference process by introducing  poisoned samples during the training process.</p>
<p><strong>Differences</strong> :</p>
<p>data poisoning aims at degrading the performance in predicting benign testing  samples;</p>
<p>backdoor attacks preserve the performance on benign samples, similarly with the  benign model, while changing the prediction of attacked samples (i.e., benign  testing samples with trigger) to the target label.</p>
<p>backdoor attacks are more malicious than data poisoning</p>
<p>existing data poisoning works have also inspired the research on backdoor  learning due to their similarities. For example, Hong et al. demonstrated that  the defense towards data poisoning may also have benefits in defending backdoor  attacks.</p>
<h2 id="BACKDOOR-DEFENSES"><a href="#BACKDOOR-DEFENSES" class="headerlink" title="BACKDOOR DEFENSES"></a>BACKDOOR DEFENSES</h2><p>Existing methods mostly aim at defending poisoning-based attacks and can be  divided into two main categories, including <strong>empirical backdoor</strong> defenses(on some understandings of existing attacks ) and  <strong>certified backdoor defenses</strong>(theoretically guaranteed under certain assumptions).</p>
<p>Empirical Backdoor Defenses</p>
<p>Certified Backdoor Defenses</p>
<h2 id="BACKDOOR-DEFENSES-1"><a href="#BACKDOOR-DEFENSES-1" class="headerlink" title="BACKDOOR DEFENSES"></a>BACKDOOR DEFENSES</h2><h3 id="Empirical-Backdoor-Defenses"><a href="#Empirical-Backdoor-Defenses" class="headerlink" title="Empirical Backdoor Defenses"></a><strong>Empirical Backdoor Defenses</strong></h3><p>three main defense paradigms, including</p>
<p>(1) trigger-backdoor mismatch, </p>
<p>(2)  backdoor elimination, and </p>
<p>(3) trigger elimination, </p>
<p>can be adopted to defend  existing attacks.</p>
<p><img src="/images/backdoor-note/2.png"></p>
<p>Preprocessing-based Defenses: </p>
<ul>
<li>Liu et al. were the first to exploit the preprocessing technique as a  defense approach towards image classification</li>
<li>a two-stage image preprocessing approach, dubbed Februus, was proposed by Doan  et al</li>
<li>Udeshi el al. proposed to utilize the dominant color in the image to make a  square-like trigger blocker in the preprocessing stage</li>
<li>Vasquez et al.proposed to preprocess the image through style transfer.</li>
<li>Li et al. discussed the property of existing poisoning-based attacks with static  trigger pattern.</li>
</ul>
<p>Model Reconstruction based Defenses:</p>
<ul>
<li>Liu et al. proposed to retrain the given model with local benign samples  starting from the weight of the given model.</li>
<li>Liu et al. proposed to prune those neurons to remove the hidden backdoor.</li>
<li>Zhao et al. showed that the hidden backdoor of the infected DNNs can be repaired  based on the mode connectivity technique with a certain amount of benign  samples</li>
<li>Most recently, Y oshida et al. and Li et al. proposed to utilize knowledge  distillation technique to reconstruct (infected) DNNs</li>
</ul>
<p>Others:</p>
<p>Trigger Synthesis based Defenses</p>
<p>Model Diagnosis based Defenses</p>
<p>Poison Suppression based Defenses</p>
<p>Training Sample Filtering based Defenses</p>
<p>Testing Sample Filtering based Defenses</p>
<h3 id="Certified-Backdoor-Defenses"><a href="#Certified-Backdoor-Defenses" class="headerlink" title="Certified Backdoor Defenses"></a>Certified Backdoor Defenses</h3><p>Almost all of empirical backdoor defenses were <strong>bypassed by following stronger adaptive attacks</strong></p>
<ul>
<li>Wang et al. took the first step towards the certified defense against backdoor  attacks based on the random smoothing technique</li>
<li>Wang et al. treated the entire training procedure of the classifier as the base  function to generalize classical randomized smoothing to defend against backdoor  attacks.</li>
<li>Weber et al. demonstrated that directly applying randomized smoothing  will not  provide high certified robustness bounds.</li>
</ul>
<h2 id="OUTLOOK-OF-FUTURE-DIRECTIONS"><a href="#OUTLOOK-OF-FUTURE-DIRECTIONS" class="headerlink" title="OUTLOOK OF FUTURE DIRECTIONS"></a>OUTLOOK OF FUTURE DIRECTIONS</h2><p>The development of this field is still in its infancy, as many critical problems  of backdoor learning have not been well studied.</p>
<p>five potential research directions: <strong>Trigger Design, Semantic and Physical Backdoor Attacks, Attacks Towards Other Tasks, Effective and Efficient Defenses, Mechanism Exploration</strong></p>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="https://yezzi.tech/2021/09/17/backdoor-learning-a-survey-note/" data-id="cktnlkhsa00034ku7hfd72u16" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/backdoor-attack/" rel="tag">backdoor attack</a></li></ul>


    </footer>
  </div>
  
    
<ul id="article-nav" class="nav nav-pills nav-justified">
  
  <li role="presentation">
    <a href="/2021/08/29/14-Bidirnctional-RNNS-and-deep-RNNs/" id="article-nav-older" class="article-nav-link-wrap">
      <i class="fa fa-chevron-left pull-left"></i>
      <span class="article-nav-link-title">14 bidirectional RNNS and deep RNNs</span>
    </a>
  </li>
  
  
  <li role="presentation">
    <a href="/2021/09/17/Backdoor-Attack-note/" id="article-nav-newer" class="article-nav-link-wrap">
      <span class="article-nav-link-title">Backdoor Attack note</span>
      <i class="fa fa-chevron-right pull-right"></i>
    </a>
  </li>
  
</ul>


  
</article>




        </div>
        <div class="col-sm-3 col-sm-offset-1 blog-sidebar">
          
  <div class="sidebar-module sidebar-module-inset">
  <h4>About</h4>
  <p> Nice papers collected in the field of <em>AI</em>, <em>Apllied Cryptography</em> and <em>Blockchain</em>, records of some computer technologies, and personal thoughts.  If you are inteseted in my field, feel free to contact me at 1721062927@qq.com </p>

</div>


  


  
  <div class="sidebar-module">
    <h4>Tags</h4>
    <ul class="sidebar-module-list" itemprop="keywords"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Bitcoin/" rel="tag">Bitcoin</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a><span class="sidebar-module-list-count">15</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/backdoor-attack/" rel="tag">backdoor attack</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/blockchain-attack/" rel="tag">blockchain attack</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/golang/" rel="tag">golang</a><span class="sidebar-module-list-count">5</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/hyperledger-fabric/" rel="tag">hyperledger fabric</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/java/" rel="tag">java</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/paper-notes/" rel="tag">paper notes</a><span class="sidebar-module-list-count">5</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/projects/" rel="tag">projects</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/%E4%BB%A5%E5%A4%AA%E5%9D%8A/" rel="tag">以太坊</a><span class="sidebar-module-list-count">18</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/%E5%89%8D%E7%AB%AF/" rel="tag">前端</a><span class="sidebar-module-list-count">1</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>Tag Cloud</h4>
    <p class="tagcloud">
      <a href="/tags/Bitcoin/" style="font-size: 10px;">Bitcoin</a> <a href="/tags/Machine-Learning/" style="font-size: 18px;">Machine Learning</a> <a href="/tags/backdoor-attack/" style="font-size: 12px;">backdoor attack</a> <a href="/tags/blockchain-attack/" style="font-size: 12px;">blockchain attack</a> <a href="/tags/golang/" style="font-size: 16px;">golang</a> <a href="/tags/hyperledger-fabric/" style="font-size: 12px;">hyperledger fabric</a> <a href="/tags/java/" style="font-size: 12px;">java</a> <a href="/tags/paper-notes/" style="font-size: 16px;">paper notes</a> <a href="/tags/projects/" style="font-size: 14px;">projects</a> <a href="/tags/%E4%BB%A5%E5%A4%AA%E5%9D%8A/" style="font-size: 20px;">以太坊</a> <a href="/tags/%E5%89%8D%E7%AB%AF/" style="font-size: 10px;">前端</a>
    </p>
  </div>


  
  <div class="sidebar-module">
    <h4>Archives</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2021/09/">September 2021</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2021/08/">August 2021</a><span class="sidebar-module-list-count">14</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2021/07/">July 2021</a><span class="sidebar-module-list-count">21</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/02/">February 2020</a><span class="sidebar-module-list-count">9</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/10/">October 2019</a><span class="sidebar-module-list-count">9</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/08/">August 2019</a><span class="sidebar-module-list-count">2</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>Recents</h4>
    <ul class="sidebar-module-list">
      
        <li>
          <a href="/2021/09/17/Backdoor-Attack-note/">Backdoor Attack note</a>
        </li>
      
        <li>
          <a href="/2021/09/17/backdoor-learning-a-survey-note/">backdoor learning-a survey note</a>
        </li>
      
        <li>
          <a href="/2021/08/29/14-Bidirnctional-RNNS-and-deep-RNNs/">14 bidirectional RNNS and deep RNNs</a>
        </li>
      
        <li>
          <a href="/2021/08/28/13-Vanishing-gradients-with-RNNs/">13 Vanishing gradients with RNNs and GRU and LSTM</a>
        </li>
      
        <li>
          <a href="/2021/08/28/12-language-model-and-sequence-generation/">12 language model and sequence generation</a>
        </li>
      
    </ul>
  </div>



        </div>
    </div>
  </div>
  <footer class="blog-footer">
  <div class="container">
    <div id="footer-info" class="inner">
      &copy; 2021 Mingzhe Liu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

  

<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha384-8gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>




<script src="/js/script.js"></script>


</body>
</html>
