<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Backdoor Attack note | Yezzi Tech</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="some backdoor attacks learned">
<meta property="og:type" content="article">
<meta property="og:title" content="Backdoor Attack note">
<meta property="og:url" content="https://yezzi.tech/2021/09/17/Backdoor-Attack-note/index.html">
<meta property="og:site_name" content="Yezzi Tech">
<meta property="og:description" content="some backdoor attacks learned">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://yezzi.tech/images/Backdoor-Attack-note/1.png">
<meta property="og:image" content="https://yezzi.tech/images/Backdoor-Attack-note/1.jpg">
<meta property="og:image" content="https://yezzi.tech/images/Backdoor-Attack-note/2.png">
<meta property="og:image" content="https://yezzi.tech/images/Backdoor-Attack-note/3.png">
<meta property="og:image" content="https://yezzi.tech/images/Backdoor-Attack-note/4.png">
<meta property="og:image" content="https://yezzi.tech/images/Backdoor-Attack-note/5.png">
<meta property="og:image" content="https://yezzi.tech/images/Backdoor-Attack-note/6.png">
<meta property="og:image" content="https://yezzi.tech/images/Backdoor-Attack-note/7.png">
<meta property="og:image" content="https://yezzi.tech/images/Backdoor-Attack-note/8.png">
<meta property="og:image" content="https://yezzi.tech/images/Backdoor-Attack-note/9.png">
<meta property="og:image" content="https://yezzi.tech/images/Backdoor-Attack-note/10.png">
<meta property="article:published_time" content="2021-09-16T23:55:24.000Z">
<meta property="article:modified_time" content="2021-09-17T00:17:23.051Z">
<meta property="article:author" content="Mingzhe Liu">
<meta property="article:tag" content="backdoor attack">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yezzi.tech/images/Backdoor-Attack-note/1.png">
  
    <link rel="alternate" href="/atom.xml" title="Yezzi Tech" type="application/atom+xml">
  
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" integrity="sha384-XdYbMnZ/QjLh6iI4ogqCTaIjrFk87ip+ekIjefZch0Y+PvJ8CDYtEs1ipDmPorQ+" crossorigin="anonymous">

  
<link rel="stylesheet" href="/css/styles.css">

  

<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <nav class="navbar navbar-inverse">
  <div class="container">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#main-menu-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="main-menu-navbar">
      <ul class="nav navbar-nav">
        
          <li><a class=""
                 href="/index.html">Home</a></li>
        
          <li><a class=""
                 href="/archives/">Archives</a></li>
        
      </ul>

      <!--
      <ul class="nav navbar-nav navbar-right">
        
          <li><a href="/atom.xml" title="RSS Feed"><i class="fa fa-rss"></i></a></li>
        
      </ul>
      -->
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

  <div class="container">
    <div class="blog-header">
  <h1 class="blog-title">Yezzi Tech</h1>
  
</div>

    <div class="row">
        <div class="col-sm-8 blog-main">
          <article id="post-Backdoor-Attack-note" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 class="article-title" itemprop="name">
      Backdoor Attack note
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2021/09/17/Backdoor-Attack-note/" class="article-date"><time datetime="2021-09-16T23:55:24.000Z" itemprop="datePublished">2021-09-17</time></a>
</div>

    
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>some backdoor attacks learned</p>
<span id="more"></span>

<h1 id="1-Invisible-Backdoor-Attack-with-Sample-Specific-Triggers"><a href="#1-Invisible-Backdoor-Attack-with-Sample-Specific-Triggers" class="headerlink" title="1 Invisible Backdoor Attack with Sample-Specific Triggers"></a>1 Invisible Backdoor Attack with Sample-Specific Triggers</h1><p>In  <em><strong>IEEE International Conference on Computer Vision (ICCV), 2021</strong></em></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>(1) We provide a comprehensive discussion about the success conditions of  current main-stream backdoor defenses. We reveal that their success all relies  on a prerequisite that backdoor triggers are sample-agnostic. </p>
<p>(2) We explore a  novel invisible attack paradigm, where the backdoor trigger is sample-specific  and invisible. It can bypass existing defenses for it breaks their fundamental  assumption. </p>
<p>(3) Extensive experiments are conducted, which verify the  effectiveness of the proposed method.</p>
<h2 id="Sample-specific-Bacckdoor-Attack"><a href="#Sample-specific-Bacckdoor-Attack" class="headerlink" title="Sample-specific Bacckdoor Attack"></a>Sample-specific Bacckdoor Attack</h2><h3 id="Threat-Model"><a href="#Threat-Model" class="headerlink" title="Threat Model"></a>Threat Model</h3><h4 id="Attacker’s-Capacities"><a href="#Attacker’s-Capacities" class="headerlink" title="Attacker’s Capacities"></a>Attacker’s Capacities</h4><ul>
<li>Attackers are allowed to poison some training data. In the inference process, attackers can and only can query the trained model  with any image.</li>
<li>They have neither information about the model nor can they manipulate the inference process.</li>
</ul>
<h4 id="Attacker’s-Goals"><a href="#Attacker’s-Goals" class="headerlink" title="Attacker’s Goals"></a>Attacker’s Goals</h4><p><strong>effectiveness</strong>: the prediction of attacked DNNs should be the target label when the backdoor  trigger appears, and the performance on benign testing samples will not be  significantly reduced</p>
<p><strong>stealthiness</strong>: adopted triggers should be concealed and the proportion of poison samples (i.e.,  the poisoning rate) should be small</p>
<p><strong>sustainability</strong>: the attack should still be effective under some common backdoor defenses</p>
<h3 id="The-Proposed-Attack"><a href="#The-Proposed-Attack" class="headerlink" title="The Proposed Attack"></a>The Proposed Attack</h3><p><strong>Definitions:</strong></p>
<p><img src="/images/Backdoor-Attack-note/1.png"></p>
<p><strong>Notations:</strong></p>
<p><img src="/images/Backdoor-Attack-note/1.jpg"></p>
<p><strong>training process</strong> :</p>
<p>The encoder takes a benign image and the representative string to generate the  poisoned image, decoder is trained to recover the hidden message from the encoded image.</p>
<p><img src="/images/Backdoor-Attack-note/2.png"></p>
<p>Note that attackers can also use other methods, such as VAE, to conduct the  sample-specific backdoor attack. It will be further studied in our future work.</p>
<p><img src="/images/Backdoor-Attack-note/3.png"></p>
<h1 id="2-Backdoor-Attacks-to-Graph-Neural-Networks"><a href="#2-Backdoor-Attacks-to-Graph-Neural-Networks" class="headerlink" title="2 Backdoor Attacks to Graph Neural Networks"></a>2 Backdoor Attacks to Graph Neural Networks</h1><p>In <strong><em>ACM Symposium on Access Control Models and Technologies (SACMAT),</em> 2021</strong> </p>
<h2 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h2><p>Graphs have been widely used to model complex interactions between entities.</p>
<p>Graph classification, which takes a graph as an input and outputs a label for  the graph, is a basic graph analytics tool and has many applications such as  fraud(欺诈) detection, malware(恶意软件) detection, and healthcare.</p>
<p>An attacker is motivated to attack GNNs to evade detection, for instance, a fake  user can attack GNNs such that it is misclassified as a genuine user.</p>
<p>In this work, ‘we’ propose the first backdoor attack to GNNs: applies the same trigger to testing graphs and does not need knowledge of the  target GNN to be successful</p>
<p>Given a trigger size and trigger density, a trigger synthesis method generates a  random subgraph that has the given size and density. </p>
<p>The attacker injects the subgraph/trigger to each poisoned training graph and  sets its label to an attacker-chosen target label.</p>
<p>‘Our’ contributions can be summarized as follows: </p>
<p>• ‘We’ perform the first  systematic study on backdoor attacks to GNNs. </p>
<p>• ‘We’ propose subgraph based  backdoor attacks to GNNs. We extensively evaluate our attacks on three  real-world datasets. </p>
<p>• ‘We’ generalize a state-of-the-art certified defense to  defend against our backdoor attacks. Our empirical results highlight the needs  of new defenses against our backdoor attacks</p>
<h2 id="Threat-Model-1"><a href="#Threat-Model-1" class="headerlink" title="Threat Model"></a>Threat Model</h2><p><strong>Attacker’s goal:</strong> </p>
<ol>
<li>the backdoor attack should not influence the GNN classifier’s accuracy on clean  testing graphs</li>
<li>the backdoored GNN classifier should predict an attacker-chosen target label for  any testing graph once a trigger is injected to the testing graph</li>
</ol>
<p><strong>Attacker’s capability</strong></p>
<p>the attacker can inject a trigger to each poisoned training graph and change its  label to an attacker-chosen target label</p>
<h2 id="Subgraph-based-Backdoor-Attacks"><a href="#Subgraph-based-Backdoor-Attacks" class="headerlink" title="Subgraph based Backdoor Attacks"></a>Subgraph based Backdoor Attacks</h2><p>Suppose a subgraph consists of t nodes. Injecting the subgraph to a graph means  that we sample t nodes from the graph uniformly at random, map them to the t  nodes in the subgraph randomly, and replace their connections as the subgraph.</p>
<p><img src="/images/Backdoor-Attack-note/4.png"></p>
<p>Designing the subgraph is key to our backdoor attack. Intuitively, the subgraph  should be unique among the clean training/testing graphs, so the backdoored GNN  is more likely to associate the target label with the subgraph.</p>
<p><strong>Trigger size and trigger density</strong></p>
<p><strong>Trigger synthesis method</strong></p>
<p><strong>Poisoning intensity</strong></p>
<h1 id="3-Graph-Backdoor"><a href="#3-Graph-Backdoor" class="headerlink" title="3 Graph Backdoor"></a><strong>3 Graph Backdoor</strong></h1><p>In  <em><strong>The 30th USENIX Security Symposium (Security ‘21)</strong></em> </p>
<p>In this paper, we seek to bridge this gap by answering the following questions:  </p>
<p>• RQ1– Are GNNs ever susceptible to backdoor attacks? </p>
<p>• RQ2– How effective are  the attacks under various practical settings (e.g., on off-the-shelf GNNs or in  input spaces)? </p>
<p>• RQ3– What are the potential countermeasures?</p>
<h3 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h3><p>We present GTA, the first backdoor attack on GNNs, which highlights with the  following features: </p>
<p>(i) it uses subgraphs as triggers; </p>
<p>(ii) it tailors trigger  to individual graphs; </p>
<p>(iii) it assumes no knowledge regarding downstream models;  </p>
<p>(iv) it also applies to both inductive and transductive tasks.</p>
<p>We empirically demonstrate that GTA is effective in a range of security-critical  tasks, evasive to detection, and agnostic to downstream models. The evaluation  characterizes the inherent vulnerabilities of GNNs to backdoor attacks.</p>
<p>We provide analytical justification for the effectiveness of GTA and discuss  potential mitigation. This analysis sheds light on improving the current  practice of re-using pre-trained GNN models, pointing to several research  directions.</p>
<p><img src="/images/Backdoor-Attack-note/5.png"></p>
<h3 id="GTA-Attack"><a href="#GTA-Attack" class="headerlink" title="GTA Attack"></a>GTA Attack</h3><p>The adversary’s objective:</p>
<p><img src="/images/Backdoor-Attack-note/6.png"></p>
<p>And the research team came up with several methods to over come challenges in Graph Attack:</p>
<p>(i) instead of associating gtand θ with final predictions, we optimize them with  respect to intermediate representations;</p>
<p>(ii) we adopt a bi-level optimization  formulation, which considers gtas the hyper-parameters and θ as the model  parameters and optimizes them in an interleaving manner; </p>
<p>(iii) we implement the  mixing function m(G;gt) as an efficient substitution operator, which finds and  replaces within G the subgraph g most similar to gt; and </p>
<p>(iv) we introduce the  concept of adaptive trigger, that is, gtis specifically optimized for each given  graph G.</p>
<p><strong>Overall framework:</strong></p>
<p><img src="/images/Backdoor-Attack-note/7.png"></p>
<p><strong>bi-level optimization objective:</strong></p>
<p><img src="/images/Backdoor-Attack-note/8.png"></p>
<p><strong>Mixing function</strong>:</p>
<p>(i) for a given trigger gt, it identifies the optimal to-be-replaced subgraph g  within a given graph G; and </p>
<p>(ii) it performs the substitution of g with gt</p>
<p><strong>Trigger generation:</strong></p>
<p>We  postulate whether it is possible to generate triggers tailored to  individual graphs to maximize the attack effectiveness and evasiveness</p>
<p><strong>Implementation and optimization</strong></p>
<p><img src="/images/Backdoor-Attack-note/9.png"></p>
<h1 id="4-BadEncoder-Backdoor-Attacks-to-Pre-trained-Encoders-in-Self-Supervised-Learning"><a href="#4-BadEncoder-Backdoor-Attacks-to-Pre-trained-Encoders-in-Self-Supervised-Learning" class="headerlink" title="4 BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning"></a>4 BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning</h1><p> <strong>In <em>IEEE Symposium on Security and Privacy,</em> 2022</strong> </p>
<h3 id="Self-Supervised-Learning"><a href="#Self-Supervised-Learning" class="headerlink" title="Self-Supervised Learning"></a>Self-Supervised Learning</h3><p>Self-supervised learning aims to pre-train an image encoder using a large amount  of unlabeled data, and the pre-trained image encoder can then be used to build  classifiers for many downstream tasks (we consider image classification tasks in  this work) with a small amount of or no labeled data.</p>
<p>The selfsupervised learning pipeline consists of two key components, i.e.,  <em>pre-training an image encoder and building a downstream classifier</em>.</p>
<p><strong>Pre-training an Image Encoder</strong></p>
<p>An image encoder is a neural network which takes an image as input and outputs a  feature vector for it. Self-supervised learning pre-trains an image encoder  using a large amount of unlabeled data which we call pre-training dataset. The  pretraining dataset could contain unlabeled images or (image, text) pairs.</p>
<p><strong>Building a Downstream Classifier</strong></p>
<p>The pre-trained image encoder can be used as a feature extractor to build  classifiers (called downstream classifiers) for many downstream tasks.</p>
<h3 id="Threat-Model-2"><a href="#Threat-Model-2" class="headerlink" title="Threat Model"></a>Threat Model</h3><p><strong>Attacker’s goals</strong></p>
<p>We consider an attacker aims to inject backdoors into a pre-trained image  encoder such that a downstream classifier built based on the backdoored image  encoder makes attacker-chosen predictions for inputs embedded with an  attacker-chosen trigger.</p>
<p><strong>Attacker’s background knowledge and capabilities</strong></p>
<p>consider two possible attackers:</p>
<ol>
<li><p>an untrusted service provider who injects a backdoor into its pre-trained  image encoder and shares the backdoored encoder with downstream customers (e.g.,  makes the backdoored encoder publicly available)</p>
</li>
<li><p>a malicious third-party who obtains a clean pre-trained image encoder from a  service provider, injects backdoors into it, and shares the backdoored encoder  with downstream customers (e.g., via republishing it for public download on  GitHub).</p>
</li>
</ol>
<h3 id="DESIGN-OF-BADENCODER"><a href="#DESIGN-OF-BADENCODER" class="headerlink" title="DESIGN OF BADENCODER"></a>DESIGN OF BADENCODER</h3><p><img src="/images/Backdoor-Attack-note/10.png"></p>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="https://yezzi.tech/2021/09/17/Backdoor-Attack-note/" data-id="cktnlkhrm00004ku72eih5x9f" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/backdoor-attack/" rel="tag">backdoor attack</a></li></ul>


    </footer>
  </div>
  
    
<ul id="article-nav" class="nav nav-pills nav-justified">
  
  <li role="presentation">
    <a href="/2021/09/17/backdoor-learning-a-survey-note/" id="article-nav-older" class="article-nav-link-wrap">
      <i class="fa fa-chevron-left pull-left"></i>
      <span class="article-nav-link-title">backdoor learning-a survey note</span>
    </a>
  </li>
  
  
</ul>


  
</article>




        </div>
        <div class="col-sm-3 col-sm-offset-1 blog-sidebar">
          
  <div class="sidebar-module sidebar-module-inset">
  <h4>About</h4>
  <p> Nice papers collected in the field of <em>AI</em>, <em>Apllied Cryptography</em> and <em>Blockchain</em>, records of some computer technologies, and personal thoughts.  If you are inteseted in my field, feel free to contact me at 1721062927@qq.com </p>

</div>


  


  
  <div class="sidebar-module">
    <h4>Tags</h4>
    <ul class="sidebar-module-list" itemprop="keywords"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Bitcoin/" rel="tag">Bitcoin</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a><span class="sidebar-module-list-count">15</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/backdoor-attack/" rel="tag">backdoor attack</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/blockchain-attack/" rel="tag">blockchain attack</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/golang/" rel="tag">golang</a><span class="sidebar-module-list-count">5</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/hyperledger-fabric/" rel="tag">hyperledger fabric</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/java/" rel="tag">java</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/paper-notes/" rel="tag">paper notes</a><span class="sidebar-module-list-count">5</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/projects/" rel="tag">projects</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/%E4%BB%A5%E5%A4%AA%E5%9D%8A/" rel="tag">以太坊</a><span class="sidebar-module-list-count">18</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/%E5%89%8D%E7%AB%AF/" rel="tag">前端</a><span class="sidebar-module-list-count">1</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>Tag Cloud</h4>
    <p class="tagcloud">
      <a href="/tags/Bitcoin/" style="font-size: 10px;">Bitcoin</a> <a href="/tags/Machine-Learning/" style="font-size: 18px;">Machine Learning</a> <a href="/tags/backdoor-attack/" style="font-size: 12px;">backdoor attack</a> <a href="/tags/blockchain-attack/" style="font-size: 12px;">blockchain attack</a> <a href="/tags/golang/" style="font-size: 16px;">golang</a> <a href="/tags/hyperledger-fabric/" style="font-size: 12px;">hyperledger fabric</a> <a href="/tags/java/" style="font-size: 12px;">java</a> <a href="/tags/paper-notes/" style="font-size: 16px;">paper notes</a> <a href="/tags/projects/" style="font-size: 14px;">projects</a> <a href="/tags/%E4%BB%A5%E5%A4%AA%E5%9D%8A/" style="font-size: 20px;">以太坊</a> <a href="/tags/%E5%89%8D%E7%AB%AF/" style="font-size: 10px;">前端</a>
    </p>
  </div>


  
  <div class="sidebar-module">
    <h4>Archives</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2021/09/">September 2021</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2021/08/">August 2021</a><span class="sidebar-module-list-count">14</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2021/07/">July 2021</a><span class="sidebar-module-list-count">21</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2020/02/">February 2020</a><span class="sidebar-module-list-count">9</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/10/">October 2019</a><span class="sidebar-module-list-count">9</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/08/">August 2019</a><span class="sidebar-module-list-count">2</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>Recents</h4>
    <ul class="sidebar-module-list">
      
        <li>
          <a href="/2021/09/17/Backdoor-Attack-note/">Backdoor Attack note</a>
        </li>
      
        <li>
          <a href="/2021/09/17/backdoor-learning-a-survey-note/">backdoor learning-a survey note</a>
        </li>
      
        <li>
          <a href="/2021/08/29/14-Bidirnctional-RNNS-and-deep-RNNs/">14 bidirectional RNNS and deep RNNs</a>
        </li>
      
        <li>
          <a href="/2021/08/28/13-Vanishing-gradients-with-RNNs/">13 Vanishing gradients with RNNs and GRU and LSTM</a>
        </li>
      
        <li>
          <a href="/2021/08/28/12-language-model-and-sequence-generation/">12 language model and sequence generation</a>
        </li>
      
    </ul>
  </div>



        </div>
    </div>
  </div>
  <footer class="blog-footer">
  <div class="container">
    <div id="footer-info" class="inner">
      &copy; 2021 Mingzhe Liu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

  

<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha384-8gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>




<script src="/js/script.js"></script>


</body>
</html>
